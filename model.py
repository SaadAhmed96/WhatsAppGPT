import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# pinecone vectorstore imports
from langchain.vectorstores import Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone

load_dotenv()

# ENV VARIABLES
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_API_ENV = os.getenv("PINECONE_API_ENV")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

# initialize the LLM
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key=OPENAI_API_KEY,
    temperature=0.7,
    max_tokens=256,
    top_p=1,
)

# initialize the index
pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)

# initialize the embedding
embed = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

text_field = "text"

# switch back to normal index for langchain
index = pinecone.Index(PINECONE_INDEX_NAME)

# connect to vector store - pinecone
vectorstore = Pinecone(index, embed.embed_query, text_field)


def get_gpt_response(incoming_msg):
    """
    This function takes in an incoming message as input and uses RetrievalQA chain and
    GPT 3.5-turbo model to generate a response to the incoming message.

    Args:
        incoming_msg (str) : The human whatsapp msg

    Returns:
        model_response (str) : The response generated by the LangChain model to the incoming message.

    """
    qa = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever()
    )

    # pass in the chatmodel
    model_response = qa.run(incoming_msg)

    return model_response
